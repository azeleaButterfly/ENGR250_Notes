\documentclass{article}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{venndiagram} 
\usepackage[margin=1in]{geometry}
\usepackage{qtree}
\usepackage{hyperref}
\hypersetup{
    colorlinks=false,
    linktoc=all,
}
\usepackage{mathtools}

\newcommand\Myperm[2][^n]{\prescript{#1\mkern-2.5mu}{}P_{#2}}
\newcommand\Mycomb[2][^n]{\prescript{#1\mkern-0.5mu}{}C_{#2}}

\title{University of the Pacific ENGR 250 Notes}
\author{Mari Anderson}   
\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Week 1}
\subsection{Set Theory}
A \textbf{set} is a collection of things. For example: 
\begin{itemize}
    \item Collection of all natural numbers $\mathbb{N} = \{1,2,3,4,5,...\}$
    \item Even natural numbers less than or equal to 6: $E = \{2,4,6\}$.
\end{itemize}
Elements of a set are denoted using lowercase letters. For example if $x=4$ belngs to $E$ we would denote that with $x \in E$. To denote an element is \textbf{not} in a set you dash the epsilon: $5 \notin E$.
\subsubsection{Set Operations}
\textbf{Set Union:} \newline
The union of two sets $A$ and $B$ is the set of all elements which is either in $A$ or $B$. Behaves similar to logical OR from digital design. Formal Definiton: $A \cup B = \{ x  | (x \in A) \lor (x \in B)\}$.
\newline
\begin{venndiagram2sets}[tikzoptions={scale=0.95}]
\fillA
\fillB
\end{venndiagram2sets}
\newline
\textbf{Set Intersection:} \newline
$A \cap B$ is the intersection of two sets, and contains every element that is both in $A$ and $B$. Behaves similarly to logical AND from digital design. Formal Defintion: $A \cap B = \{ x | (x \in A) \land (x \in B)\}$
\newline
\begin{venndiagram2sets}[tikzoptions={scale=0.95}]
\fillACapB
\end{venndiagram2sets}\newline
\textbf{Set Compliment:}
$A^c$ is the compliment of $A$ and contains every element not in $A$. Behaves similar to logical NOT from digital design. Formal Definition: $A^c = \{x | x \notin A\}$.
\newline
\begin{venndiagram2sets}[tikzoptions={scale=0.95}]
    \fillNotA
\end{venndiagram2sets}
\newline
\textbf{Set Difference:}
$A - B = A \cap B^c$. Contains every element of $A$ that is not in $B$. \newline
\begin{venndiagram2sets}[tikzoptions={scale=0.95}]
\fillOnlyA
\end{venndiagram2sets}

\subsubsection{Other Definitions}
A collection of sets $A_1, ...., A_n$ is \textbf{mutually exlusive} if and only if:
\[
    A_i \cap A_j = \emptyset \hspace{5mm} i \neq j 
\]
A collection of sets $A_1, \dots, A_n$ is \textbf{collectively exhaustive} if and only if 
\[
A_1 \cup A_2 \cup \dots \cup A_n = S
\]
Two sets are equal to each other if and only if 
\[
    (A \subseteq B) \land (B \subseteq A)
\]
\textbf{De Morgan's Law:} De Morgan's law relates all three basic set operations
\[
    (A \cup B)^c = A^c \cap B^c
\]
\textbf{Proof:} Let $x \in (A \cup B)^c$. Then as $x \notin A \cup B$ therefore $x \notin A$ and $x \notin B$ Therefore $x \in A^c \cap B^c$ Therefore $(A \cup B)^c \subseteq A^c \cap B^c$. Now assume $x \in A^c \cap B^c$. Then $x \notin A$ and $x \notin B$, and therefore $x \notin (A \cup B)$. Thus $x \in (A \cup B)^c$. Therefore $(A \cup B)^c = A^c \cap B^c$. $\square$
\[
 (A \cap B)^C = A^c \cup B^c
\]
\textbf{Proof:} Let $x \in (A \cap B)^c$. Then $x$ is either in $A$ not in $B$, in $B$ not in $A$, or not in either $A$ or $B$. Therefore $x \in A^c \cup B^c$ and thus $(A \cap B)^c \subseteq A^c \cup B^c$. Now let $x \in A^c \cup B^c$. Then by definition $x$ is either in $A^c$ or $B^c$. Thus $x$ is not in both $A$ and $B$. Therefore $x \in (A \cap B)^c$ and thus $A^c \cup B^c \subseteq (A \cap B)^c$. As $A^c \cup B^c \subseteq (A \cap B)^c$ and $(A \cap B)^c \subseteq A^c \cup B^c$, $(A\cap B)^c = A^c \cup B^c$. $\square$ 

\subsection{Applying Set Theory to Probability}
An \textbf{experiment} consists of a procedure and observations.

\begin{center}
\begin{tabular}{c|c|c}
    Experiment & Procedure & Observation \\
    \hline
    Coin Flip & Flip the coin & heads or tails \\
    Dice Rolls & Roll the die & the number face up on the die \\
    Networking & Send packets & Record the packets that successfully get transmitted
\end{tabular}
\end{center}
The \textbf{sample space} of an experiment is the finest-grain, mutually exclusive, collectively exhaustive set of all possible outcomes.  

\begin{center}
    \begin{tabular}{c|c}
        Roll a die & $S = D = \{1,2,3,4,5,6\}$ \\
        Flip a coin & $S = C = \{H,T\}$ \\
        Flip a coin twice & $S = F = \{HH, HT, TH, TT\}$
    \end{tabular}
\end{center}
An \textbf{event} is a set of desired outcomes of an experiment. Example: Roll a die, you win if you roll an even number. $E = \{2,4,6\}$.

\subsection{Axioms}
Probability P maps the events from a sample space to real numbers such that
\begin{enumerate}
    \item $P(A) \geq 0$ where $A$ is an event in the sample space $S$
    \item $P(S) = 1$ where $S$ is the universal set 
    \item For a countable collections of mutually exclusive sets $A_1, A_2, A_3, ... A_n \in S$, $P(A_1 \cup A_2 \cup \dots \cup A_n) = P(A_1) + P(A_2) + \dots + P(A_n)$

\end{enumerate}
\subsection{Theorems}
\textbf{Theorem 1.4} \newline
$P(A^c) = 1 - P(A)$. 

For any two sets $A$ and $B$ not necessarily mutually exclusive:
\[
P(A \cup B) = P(A) + B(B) - P(A \cap B)
\]
Visual Explaination:

\begin{venndiagram2sets}
\fillA
\end{venndiagram2sets}

\begin{venndiagram2sets}
\fillB
\end{venndiagram2sets}


Here we see the intersection of $A$ and $B$ could be counted twice if we add $P(A)$ and $P(B)$ so we have to subtract the intersection so it is only counted once. \newline \newline
\textbf{Theorem 1.5} \newline
The probability of event $B =  \{s_1, s_2, \dots s_m\}$ is the sum of probabilities contained in the event:
\[
    P(B) = \sum_{i=1}^{m} P(\{s_i\})
\]
Follows from axiom 3 as each $s_i$ is mutually exclusive.  \newline \newline
\textbf{Theorem 1.6} \newline
For an experiment with sample space $S = \{s_1, \dots, s_n \}$ in which each outcome $s_i$ is equally likely, 
\[
    P(s_i) = 1/n \hspace{5mm} 1 \leq i \leq n
\]
Where $n$ is the number of outcomes in the sample space (same as n is equal to the cardinality of $S$)  \newline
\newline
\textbf{Theorem 1.7} \newline
If outcomes in an experiment are equally likely, then probability of event A is given by:
\[
P(A) = \frac{|A|}{|S|}
\]
Where $||$ denotes the cardinality of the set.
\subsection{Conditional Probability}
The \textbf{conditional probability} of the event $A$ given the occurance occurance of the event $B$ is 
\[
P(A | B) = \frac{P(A \cap B)}{B}
\]
\subsection{Law of Total Probability}
\textbf{Theorem 1.10: } For an event space $\{B_1, B_2, \dots, B_m\}$ with $P(B_i) > 0$ for all $i$
\[
P(A) = \sum_{i=1}^{m} P(A | B_i)P(B_i)
\]
\newpage
\section{Week 2}
\subsection{Problem 2.4} You have two biased coins. Coin A comes up with heads with probability 0.25. Coin B comes up heads with probability 0.75. However you are not sure which is which so you choose a coin randomly and you flip it. If the flip is heads you guess Coin B. If tails you guess Coin A. What is the probability $P(C)$ that your guess is correct.
\begin{center}
\Tree[. [.CoinA(0.5) [.Heads(0.25) [.Wrong \textit{1/8} ] ]
               [.Tails(0.75) [.Correct \textit{3/8} ]]]
          [.CoinB(0.5) [.Heads(0.75) [.Correct \textit{3/8} ]]
                [.Tails(0.25) [.Wrong \textit{1/8} ]]]
                ]
\end{center}
\subsection{Problem 2.5:} 
Traffic enginers have coordinated the timing of two traffic lights to encourage the run of green lights. With probability of $0.8$ a driver will find the 2nd light to have the same color as the first. Assuming that the first light is equally likely to be red or green:
\begin{center}
\Tree[.
        [.Green(0.5)
            [.Green(0.8) \textit{0.4} ]
            [.Red(0.2) \textit {0.1} ]] 
            [.Red(0.5) 
                [.Green(0.2) \textit{0.1} ]
                [.Red(0.8) \textit{0.4} ]]
]
\end{center}
Part A: What is the probability $P(G2)$ that the second light is green? 
\[
P(G2) = P(G2 \cap G1) + P(G2 \cap R1) = 0.4 + 0.1 = 0.5
\]
Part B: What is the probability $P(W)$ that you wait for at least one of the first two lights:
\[
P(W) = P(R2 \cap G1) + P(R1) = 0.1 + 0.5 = 0.6
\]
Part C: What is $P(G1| R2)$. 
\[
P(G1 | R2) = \frac{P(G1 \cap R2)}{P(R2)} = 0.1/0.5 = 0.2
\]
\subsection{Problem 2.6:} 
You have a shuffled deck of cards labeled 2 to 12. You also have two fair dice. You toss a baised coin (heads with probability 0.75). If the result isheads then you draw a card from the shuffled deck of cards. Otherwise, you roll two dice and add the numbers. What's the probability that you get a 7 or 8.
\begin{center}
\Tree[.
        [.Heads(3/4) [{Draw 7 or 8 (2/11)} ]
            {Don't draw 7 or 8 (9/11)} ]
            [.Tails(1/4) {Roll 7 or 8 (11/36)}
                {Don't roll 7 or 8 (25/36)} ]
]
\[
P(7/8) = P(H \cap (7/8)) + P(T \cap (7/8)) = \bigg(\frac{3}{4}\bigg)\bigg(\frac{2}{11}\bigg) + \bigg(\frac{1}{4}\bigg)\bigg(\frac{11}{36}\bigg) = \frac{6}{44} + \frac{11}{144} = 0.213
\]
\end{center}
\subsection{Counting Principles}
\subsubsection{Counting Principle 1}
An experiment consists of two subexperiments. If one subexperiment hsa $k$ outcomes and the other subexperiment has $n$ outcomes, the the experiment has $nk$ outcomes.
\subsubsection{Counting Principle 2}
A sampling without replacement technique where you pick $k$ objects out of $n$ distinguishable bjects such that the order of picking does matter.
\[
P(n,k) = \Myperm[n]{k} = \frac{n!}{(n-k)!}
\]
\subsubsection{Counting Principle 3}
When you pick $k$ objects from $n$ objections, each way contains $k$ objects that can be permuted $k!$ ways. The number of ways to \textbf{choose} $k$ objects out of $n$ distinguishable objects is:
\[
\binom{n}{k} = \frac{n!}{(n-k)!k!}
\]
\subsection{Problem 2.7}
In a game of yummy you are dealt a seven card hand.
\subsubsection{Part A}
\textbf{Q:} what is the probability $P(R_7)$ that your hand only has red cards.
\[
P(R_7) = \frac{\binom{26}{7}}{\binom{52}{7}} = \frac{26!}{(26 - 7)!7!} \cdot \frac{(52-7)!7!}{52!} = \frac{26!45!}{19!52!} = 0.00492
\]
\end{document}!

