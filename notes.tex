\documentclass{article}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{venndiagram, pgfplots} 
\usepackage[margin=0.90in]{geometry}
\usepackage{qtree}
\usepackage{hyperref}
\hypersetup{
    colorlinks=false,
    linktoc=all,
}
\usepackage{mathtools}
\pgfplotsset{compat=1.18}
\newcommand\Myperm[2][^n]{\prescript{#1\mkern-2.5mu}{}P_{#2}}
\newcommand\Mycomb[2][^n]{\prescript{#1\mkern-0.5mu}{}C_{#2}}

\title{University of the Pacific ENGR 250 Notes}
\author{Mari Anderson}   
\begin{document}
\maketitle
\tableofcontents
\newpage
\section{Week 1}
\subsection{Set Theory}
A \textbf{set} is a collection of things. For example: 
\begin{itemize}
    \item Collection of all natural numbers $\mathbb{N} = \{1,2,3,4,5,...\}$
    \item Even natural numbers less than or equal to 6: $E = \{2,4,6\}$.
\end{itemize}
Elements of a set are denoted using lowercase letters. For example if $x=4$ belngs to $E$ we would denote that with $x \in E$. To denote an element is \textbf{not} in a set you dash the epsilon: $5 \notin E$.
\subsubsection{Set Operations}
\textbf{Set Union:} \newline
The union of two sets $A$ and $B$ is the set of all elements which is either in $A$ or $B$. Behaves similar to logical OR from digital design. Formal Definiton: $A \cup B = \{ x  | (x \in A) \lor (x \in B)\}$.
\newline
\begin{venndiagram2sets}[tikzoptions={scale=0.95}]
\fillA
\fillB
\end{venndiagram2sets}
\newline
\textbf{Set Intersection:} \newline
$A \cap B$ is the intersection of two sets, and contains every element that is both in $A$ and $B$. Behaves similarly to logical AND from digital design. Formal Defintion: $A \cap B = \{ x | (x \in A) \land (x \in B)\}$
\newline
\begin{venndiagram2sets}[tikzoptions={scale=0.95}]
\fillACapB
\end{venndiagram2sets}\newline
\textbf{Set Compliment:}
$A^c$ is the compliment of $A$ and contains every element not in $A$. Behaves similar to logical NOT from digital design. Formal Definition: $A^c = \{x | x \notin A\}$.
\newline
\begin{venndiagram2sets}[tikzoptions={scale=0.95}]
    \fillNotA
\end{venndiagram2sets}
\newline
\textbf{Set Difference:}
$A - B = A \cap B^c$. Contains every element of $A$ that is not in $B$. \newline
\begin{venndiagram2sets}[tikzoptions={scale=0.95}]
\fillOnlyA
\end{venndiagram2sets}

\subsubsection{Other Definitions}
A collection of sets $A_1, ...., A_n$ is \textbf{mutually exlusive} if and only if:
\[
    A_i \cap A_j = \emptyset \hspace{5mm} i \neq j 
\]
A collection of sets $A_1, \dots, A_n$ is \textbf{collectively exhaustive} if and only if 
\[
A_1 \cup A_2 \cup \dots \cup A_n = S
\]
Two sets are equal to each other if and only if 
\[
    (A \subseteq B) \land (B \subseteq A)
\]
\textbf{De Morgan's Law:} De Morgan's law relates all three basic set operations
\[
    (A \cup B)^c = A^c \cap B^c
\]
\textbf{Proof:} Let $x \in (A \cup B)^c$. Then as $x \notin A \cup B$ therefore $x \notin A$ and $x \notin B$ Therefore $x \in A^c \cap B^c$ Therefore $(A \cup B)^c \subseteq A^c \cap B^c$. Now assume $x \in A^c \cap B^c$. Then $x \notin A$ and $x \notin B$, and therefore $x \notin (A \cup B)$. Thus $x \in (A \cup B)^c$. Therefore $(A \cup B)^c = A^c \cap B^c$. $\square$
\[
 (A \cap B)^C = A^c \cup B^c
\]
\textbf{Proof:} Let $x \in (A \cap B)^c$. Then $x$ is either in $A$ not in $B$, in $B$ not in $A$, or not in either $A$ or $B$. Therefore $x \in A^c \cup B^c$ and thus $(A \cap B)^c \subseteq A^c \cup B^c$. Now let $x \in A^c \cup B^c$. Then by definition $x$ is either in $A^c$ or $B^c$. Thus $x$ is not in both $A$ and $B$. Therefore $x \in (A \cap B)^c$ and thus $A^c \cup B^c \subseteq (A \cap B)^c$. As $A^c \cup B^c \subseteq (A \cap B)^c$ and $(A \cap B)^c \subseteq A^c \cup B^c$, $(A\cap B)^c = A^c \cup B^c$. $\square$ 

\subsection{Applying Set Theory to Probability}
An \textbf{experiment} consists of a procedure and observations.

\begin{center}
\begin{tabular}{c|c|c}
    Experiment & Procedure & Observation \\
    \hline
    Coin Flip & Flip the coin & heads or tails \\
    Dice Rolls & Roll the die & the number face up on the die \\
    Networking & Send packets & Record the packets that successfully get transmitted
\end{tabular}
\end{center}
The \textbf{sample space} of an experiment is the finest-grain, mutually exclusive, collectively exhaustive set of all possible outcomes.  

\begin{center}
    \begin{tabular}{c|c}
        Roll a die & $S = D = \{1,2,3,4,5,6\}$ \\
        Flip a coin & $S = C = \{H,T\}$ \\
        Flip a coin twice & $S = F = \{HH, HT, TH, TT\}$
    \end{tabular}
\end{center}
An \textbf{event} is a set of desired outcomes of an experiment. Example: Roll a die, you win if you roll an even number. $E = \{2,4,6\}$.

\subsection{Axioms}
Probability P maps the events from a sample space to real numbers such that
\begin{enumerate}
    \item $P(A) \geq 0$ where $A$ is an event in the sample space $S$
    \item $P(S) = 1$ where $S$ is the universal set 
    \item For a countable collections of mutually exclusive sets $A_1, A_2, A_3, ... A_n \in S$, $P(A_1 \cup A_2 \cup \dots \cup A_n) = P(A_1) + P(A_2) + \dots + P(A_n)$

\end{enumerate}
\subsection{Theorems}
\textbf{Theorem 1.4} \newline
$P(A^c) = 1 - P(A)$. 

For any two sets $A$ and $B$ not necessarily mutually exclusive:
\[
P(A \cup B) = P(A) + B(B) - P(A \cap B)
\]
Visual Explaination:

\begin{venndiagram2sets}
\fillA
\end{venndiagram2sets}

\begin{venndiagram2sets}
\fillB
\end{venndiagram2sets}


Here we see the intersection of $A$ and $B$ could be counted twice if we add $P(A)$ and $P(B)$ so we have to subtract the intersection so it is only counted once.
\subsubsection{Theorem 1.5}
The probability of event $B =  \{s_1, s_2, \dots s_m\}$ is the sum of probabilities contained in the event:
\[
    P(B) = \sum_{i=1}^{m} P(\{s_i\})
\]
Follows from axiom 3 as each $s_i$ is mutually exclusive. 
\subsubsection{Theorem 1.6}
For an experiment with sample space $S = \{s_1, \dots, s_n \}$ in which each outcome $s_i$ is equally likely, 
\[
    P(s_i) = 1/n \hspace{5mm} 1 \leq i \leq n
\]
Where $n$ is the number of outcomes in the sample space (same as n is equal to the cardinality of $S$)  
\subsubsection{Theorem 1.7}
If outcomes in an experiment are equally likely, then probability of event A is given by:
\[
P(A) = \frac{|A|}{|S|}
\]
Where $||$ denotes the cardinality (number of elements) of the set.
\subsection{Conditional Probability}
The \textbf{conditional probability} of the event $A$ given the occurance occurance of the event $B$ is 
\[
P(A | B) = \frac{P(A \cap B)}{B}
\]
\subsection{Law of Total Probability}
You use the law of total probability when your desired outcome may come from muliple sources with certain probability and the sources themselves have probability of being chosen.
\subsubsection{Theorem 1.10: }
For an event space $\{B_1, B_2, \dots, B_m\}$ with $P(B_i) > 0$ for all $i$
\[
P(A) = \sum_{i=1}^{m} P(A | B_i)P(B_i)
\]
This can be shown using the conditional probability formula as 
\[
P(A) = \sum_{i=1}^{m} P(A | B_i)P(B_i) = \sum_{i=1}^{m} \frac{A\cap B_i}{B_i}P(B_i) = \sum_{i=1}^{m} P(A \cap B_i)
\]
\subsubsection{Example: Problem 2.2} 
\textbf{Problem:} A company has three machines $B_1, B_2, B_3$ making 1k resistors. It is observed that $80\%$ of the resitors by $B_1$, $90\%$ of resistors by $B_2$ and $60\%$ of resistors are acceptable. Each hour $B_1$ produces 3000 resistors, $B_2$ produces 4000 resistors, and $B_3$ produces 3000 resistors. What is the probability that the company ships an acceptable resistor. \newline
\textbf{Solution:} Let $P(B_i)$ be the probability of a resistor coming from a specific machine, and let $P(A_i)$ be the probability that machine produced an acceptable resistor. Then from the problem statement we get that $10000 = 3000 + 4000 + 3000$ resistors are produced every hour, which gives us our $P(B_i)$'s as
\[
P(B_1) = \frac{3000}{10000} = 0.3 \hspace{5mm} P(B_2) = \frac{4000}{10000} = 0.4 \hspace{5mm} P(B_3) = \frac{3000}{10000} = 0.3 
\]
Then also from the problem statement we get our $A_i$'s to be that 
\[
P(A_1) = 0.9 \hspace{5mm} P(A_2) = 0.8 \hspace{5mm} P(A_3) = 0.6
\]
From this we can compute our probability of an acceptable resistor $P(R)$ as 
\[
P(R) = P(B_1)P(A_1) + P(B_2)P(A_2) + P(B_3)P(A_3) = (0.3)(0.9) + (0.4)(0.8) + (0.3)(0.6) = 0.27 + 0.32 + 0.18 = 0.77 
\]
\subsection{Bayes' theorem}
\subsubsection{Theorem 1.11}
Bayes's theorem is that the probability of an event $B$ given event $A$ occured can be given by
\[
P(B | A) = \frac{P(A|B)P(B)}{P(A)}
\]
Which we can easily confirm as $P(A|B)P(B) = P(A \cap B) = P(B \cap A)$ and $P(B|A) = \frac{P(B \cap A)}{P(A)}$ by defintion. 
\subsubsection{Example: Problem 2.3} 
\textbf{Problem:} In the previous problem (2.2) what is the probability that the acceptable resistor came from $B_3$. \newline
\textbf{Solution:} From Problem 2.2 as $P(R | P(B_3)) = 0.60$ as 0.60 of the resistors produced by $B_3$ are acceeptable. From this we can apply bayes' theorem to get
\[
P(B_3 | R) = \frac{P(R|B_3)(P(B_3))}{P(R)} = \frac{0.18}{0.77} \approx 0.234
\]
\subsection{Independence}
\textbf{Definition:} Two events are independent if and only if 
\[
P(A \cap B) = P(A)P(B)
\]
\textbf{Defintion:} Three events $A_1, A_2$ and $A_3$ are independent if and only if 
\begin{enumerate}
    \item $A_1$ and $A_2$ are independent
    \item $A_2$ and $A_3$ are independent
    \item $A_1$ and $A_3$ are independent
    \item $P(A_1 \cap A_2 \cap A_3) = P(A_1)P(A_2)P(A_3)$
\end{enumerate}
\textbf{Definition:} Multiple events $A_1, A_2, \dots, A_n$ are independent if and only if both 
\[
P(A_i \cap P_j) = P(A_i)P(A_j) \text{ and } P(\bigcap_{i = 1}^{n} A_i) = \prod_{i=1}^{n}P(A_i)
\]
For all pairs $i,j$ $1 \leq i \leq n$, $1 \leq j \leq n$ where $i \neq j$.
\subsection{Independent Trials}
\textbf{Properties:}
\begin{enumerate}
    \item They are identical subexperiments in a sequential experiment
    \item The probability models of all subexperiments are identical and independent of the outcomes in other subexperiments (iid)
\end{enumerate}
If you have an event that is $S = \{0,1\}$ and you want to find the outcome of $k$ sucesses in $n$ trials with $P(1) = p$ in a specific ordering
\[
P(n_1 = k) = p^k(1-p)^{n-k}  
\]
If you don't care about the order of the successes you will add a binomial term, as you will be choosing $k$ successes out of $n$ trials  
\[
P(n_1 = k) = \binom{n}{k} p^k(1-p)^{n-k}
\]
The probability of at least $k$ successes in $n$ trials with each event having probability of sucesses $p$ is
\[
P(n_1 \geq k) = \sum_{i = 0}^{n-k} \binom{n}{k + i} (p)^{k+i}(1-p)^{n-k-i}
\]
Extending this to the multinomial case we get that if a sub experiment has sample spaces $S = \{s_0, \dots s_{m-1}\}$, the probability of $s_0$ appearing $n_0$ times, $s_1$ appearing $n_1$ times and $s_m$ appearing $n_m$ times is:
\[
P(E_{n_0,n_1,\dots,n_m}) = \binom{n}{n_0,n_1,\dots,n_m} P(s_0)^{n_0}P(s_1)^{n_1} \dots P(s_m)^{n_m}
\]
Where $n = \sum_{i = 0 }^{m} n_i$.
\subsection{Problem 3.1}
To communicate one bit information reliably, cellular phones trasmit the same binary symbol 5 times. The reciever detects the correct information if three or more binary symols are received correctly. What is the information error probability $P(E)$ if the binary symbol error is probability $q = 0.1$
\[
P(E) = 1 - \binom{5}{3}(0.9)^3(0.1)^2 + \binom{5}{4}(0.9)^4(0.1) + (0.9)^5 = \binom{5}{1}(0.9)(0.1)^4 + \binom{5}{2} (0.9)^2(0.1)^3
\]
\subsection{Problem 3.2}
There are 8 VMs in a lab. Each computer can be in idle (I) with probability $0.2$, suspended (S) with probability $0.2$ and probability of active (A) with probability $0.6$
What is the probability there are an equal number of idle and suspended VMs and at least 4 active VMs?
\[
P(I=S \land A \geq 4) = P(I=2, S=2, A=4) + P(I=1,S=1,A=6) + P(I=0, S=0, A = 8)
\]
\[
= \binom{8}{2,2,4} (0.2)^2(0.2)^2(0.6)^4 + \binom{8}{1,1,6}(0.2)(0.2)(0.6)^6 + \binom{8}{0,0,8} (0.6)^8
\]
\newpage
\section{Week 2}
\subsection{Problem 2.4} You have two biased coins. Coin A comes up with heads with probability 0.25. Coin B comes up heads with probability 0.75. However you are not sure which is which so you choose a coin randomly and you flip it. If the flip is heads you guess Coin B. If tails you guess Coin A. What is the probability $P(C)$ that your guess is correct.
\begin{center}
\Tree[. [.CoinA(0.5) [.Heads(0.25) [.Wrong \textit{1/8} ] ]
               [.Tails(0.75) [.Correct \textit{3/8} ]]]
          [.CoinB(0.5) [.Heads(0.75) [.Correct \textit{3/8} ]]
                [.Tails(0.25) [.Wrong \textit{1/8} ]]]
                ]
\end{center}
\subsection{Problem 2.5:} 
Traffic enginers have coordinated the timing of two traffic lights to encourage the run of green lights. With probability of $0.8$ a driver will find the 2nd light to have the same color as the first. Assuming that the first light is equally likely to be red or green:
\begin{center}
\Tree[.
        [.Green(0.5)
            [.Green(0.8) \textit{0.4} ]
            [.Red(0.2) \textit {0.1} ]] 
            [.Red(0.5) 
                [.Green(0.2) \textit{0.1} ]
                [.Red(0.8) \textit{0.4} ]]
]
\end{center}
Part A: What is the probability $P(G2)$ that the second light is green? 
\[
P(G2) = P(G2 \cap G1) + P(G2 \cap R1) = 0.4 + 0.1 = 0.5
\]
Part B: What is the probability $P(W)$ that you wait for at least one of the first two lights:
\[
P(W) = P(R2 \cap G1) + P(R1) = 0.1 + 0.5 = 0.6
\]
Part C: What is $P(G1| R2)$. 
\[
P(G1 | R2) = \frac{P(G1 \cap R2)}{P(R2)} = 0.1/0.5 = 0.2
\]
\subsection{Problem 2.6:} 
You have a shuffled deck of cards labeled 2 to 12. You also have two fair dice. You toss a baised coin (heads with probability 0.75). If the result isheads then you draw a card from the shuffled deck of cards. Otherwise, you roll two dice and add the numbers. What's the probability that you get a 7 or 8.
\begin{center}
\Tree[.
        [.Heads(3/4) [{Draw 7 or 8 (2/11)} ]
            {Don't draw 7 or 8 (9/11)} ]
            [.Tails(1/4) {Roll 7 or 8 (11/36)}
                {Don't roll 7 or 8 (25/36)} ]
]
\[
P(7/8) = P(H \cap (7/8)) + P(T \cap (7/8)) = \bigg(\frac{3}{4}\bigg)\bigg(\frac{2}{11}\bigg) + \bigg(\frac{1}{4}\bigg)\bigg(\frac{11}{36}\bigg) = \frac{6}{44} + \frac{11}{144} = 0.213
\]
\end{center}
\subsection{Counting Principles}
\subsubsection{Counting Principle 1}
An experiment consists of two subexperiments. If one subexperiment hsa $k$ outcomes and the other subexperiment has $n$ outcomes, the the experiment has $nk$ outcomes.
\subsubsection{Counting Principle 2}
A sampling without replacement technique where you pick $k$ objects out of $n$ distinguishable bjects such that the order of picking does matter.
\[
P(n,k) = \Myperm[n]{k} = \frac{n!}{(n-k)!}
\]
\subsubsection{Counting Principle 3}
When you pick $k$ objects from $n$ objections, each way contains $k$ objects that can be permuted $k!$ ways. The number of ways to \textbf{choose} $k$ objects out of $n$ distinguishable objects is:
\[
\binom{n}{k} = \frac{n!}{(n-k)!k!}  
\]


\subsection{Problem 2.7}
In a game of yummy you are dealt a seven card hand.
\subsubsection{Part A}
\textbf{Q:} what is the probability $P(R_7)$ that your hand only has red cards.
\[
P(R_7) = \frac{\binom{26}{7}}{\binom{52}{7}} = \frac{26!}{(26 - 7)!7!} \cdot \frac{(52-7)!7!}{52!} = \frac{26!45!}{19!52!} = 0.00492
\]
\subsubsection{Part B}
\textbf{Q:} What is the probability $P(F)$ that your hand has only face cards
\[
P(F) = \frac{\binom{12}{7}}{\binom{52}{7}} = 5.91 \times 10^{-6} 
\]
\subsection{Problem 2.8}
\subsubsection{Part A}
\textbf{Q:} In a game of poker you are dealt a five of card hand. What is the probability of a full house: "Three of a kind and two of a kind"
\[
P(FH) = \frac{13\binom{4}{3} \cdot 12 \binom{4}{2}}{\binom{52}{5}} = 0.00144 = 0.14\%
\]
This comes from there being 13 suites and you pick one of them, then choose 3 from the 4 cards in that suite, then for the pair you have a different suite and you pick 2 from the 4 in that new suite. Note $\binom{n}{1} = n$. 
\subsubsection{Part B}
\textbf{Q:} In a game of poker you are dealt a five of card hand. What is the probability of a 4 of a kind 
\[
P(4oK) =  \frac{13 * 48}{\frac{52}{5}} = 0.000240 = 0.02\%
\]
This comes from there being 13 ways to make a 4 of a kind, and then 48 different cards remaining in your deck of 52
\subsection{Counting Principle Number 4}
The number of observation sequences for $n$ subexperiments with sample space $S=\{0,1\}$ with $0$ appearing $n_0$ times and $1$ appearing $n_1 = n - n_0$ times is 
\[
\binom{n}{n_1}
\]
\subsubsection{Example Problem:}
Consider a 32 digit binary value, and you test each bit. How many binary codes with exactly 8 1s exist. 
\[
P(n_1 = 8) = \binom{32}{8}
\]
We get this as $n =32$ from the fact its a 32 bit value and each bit is a $0$ or $1$ value. 
\subsection{Counting Principle 5 (Multinomial)}
For $n$ repeating subexperiments with Sample Space $S - \{s_0, \dots, s_{m-1}\}$ the number of length $n=n_0 + n_1 + \dots + n_{m-1}$ obersvation sequences with $s_i$ appearing $s_i$ times is 
\[
\binom{n}{n_0, n_1, \dots, n_{m-1}} = \frac{n!}{n_0!n_1!\dots n_{m-1}!}
\]
\subsection{Problem 2.10}
\textbf{Q:} Consider testing each of the 16 elements where each element can be in high voltage state (H), low holtage state (L) and high impedance state (Z). In how many ways can you observe $6H$, $8L$ and $2Z$ elements?
\[
P(6H,8L,2Z) = \binom{16}{6,8,2} = \frac{16!}{6!8!2!}
\]
\newpage

\section{Week 3}
\subsection{Discrete Random Variables}

\newpage
\section{Week 4}
\subsection{Problem 4.6}
\textbf{Q.} A radio station gives a pair of concert tickets to the sixth $(k^{th})$ caller who knwos the birthday of the performer. For each person who calls the probability $p = 0.75$ of knowing the performer's birthday. All calls are independent. 
\subsubsection{part a}
\textbf{Q.} What is the \textbf{PMF of $L$}, the number of calls necessary to find the winner?
\[
    P_L(\ell) = \binom{L-1}{k-1}p^k(1-p)^{L-k}, \hspace{5mm} L \leq k
\]

\subsubsection{part b}
\textbf{Q:} What is the probability of calling the winner on the tenth call?  \newline
\textbf{Solution:} 
\[
    P(L = 10) = \bigg( \binom{9}{5} p^5(1-p)^4 \bigg) p = \binom{9}{5} p^6(1-p)^4 = \binom{9}{5} (0.75)^6(0.25)^4
\]

\subsubsection{part c}
\textbf{Q:} What is the probability that the station will need nine or more calls to find a winner? \newline
\textbf{Solution:}
\[
P( L \geq 9) = 1 - P(L < 9) = 1 - \bigg( P(L = 6) + P(L=7) + P(L=8) \bigg)
\]
\[
    1 - \bigg( P(L=6) + P(L=7) + P(L=8) \bigg) = 1 - \bigg( (0.75)^6 + \binom{6}{5} (0.75)^6(0.25) + \binom{7}{5}(0.75)^6(0.25)^2  \bigg) =  
\]
\subsection{Pascal Random Variable}
Consider an experimentwhere you perform $L$ Bernouli trials until you obtain $k$ of the desired result. $L$ is a Pascal Random Variable whose PMF takes the Formal
\[
P(X = x) = P_X(x) = 
\]

\subsection{Discrete Uniform Random Variable}
X is a discrete uniform random variable if the PMF of $X$ has the form:
\[
    P(X = x) = P_X(x) = = \frac{1}{l-k+1} , \hspace{5mm} k \leq x \leq l 
\]
Otherwise
\[
    P(X = x) = P_X(x) = 0
\]

\subsection{Possion Random Variable}
$X$ is Poisson $(\alpha)$ random vairable if the PMF of $X$ has the form:
\[
    P(X = x) = P_X(x) = \frac{\alpha^x e^{-\alpha}}{x!}, \hspace{3mm}x \in \mathbb{N}, \hspace{3mm} \alpha > 0
\]
Otherwise $P(X = x) = 0$
Given the rate of arrival $\lambda$ and the time interval $T$, $\alpha = \lambda T$.

\subsection{Problem 4.9}
\textbf{Q:} The number of packets at a router in any time interval is a Possion random vairable. A particular router gets 2 packets per second. 
\subsubsection{Part A}
\textbf{Q:} What is the probability there are no packets in an interval of 0.25 seconds. \newline
\textbf{Solution:} Let $x$ be the number of arrivals. We then get $\alpha = 2\times0.25 = 0.5$ as there are 2 packets over 1 second, and we are curious about how many packets arrive in 0.25 seconds. Applying this to the PMF of a Poisson RV 
\[
    P(X = 0) =  \frac{(0.5)^0(e^{-0.5})}{0!} = e^{-0.5}
\]
\subsubsection{Part B}
\textbf{Q:} What is the probability that there are no more than two packets in an interval of two seconds \newline
\textbf{Solution:} Let $x$ be the number of arrivals. Then we get that $\alpha = (2)(2) = 4$. Which then gives us the poisson PMF
\[
    P(X = x) = \frac{4^x e^{-4}} {x!}
\]
Then we can get that $P$ of no more than two is equal to 
\[
    P(X \geq 2) = P(X = 0) + P(X = 1) + P(X = 2) = e^{-4} + 4e^{-4} + 8e^{-4} = 13e^{-4}
\]
\subsection{General Advice}
For a given problem:
\begin{itemize}
    \item Deriving the PMF first without any assumptions is always good
    \item Does the experiment consist of multiple Bernoulli trials?
    \item If \textbf{YES}
        \begin{itemize}
    \item $N$ trials until and up to first desired out come $N$ is geometric RV $(p)$
    \item $n$ trials and youre looking for $X$ desired outcomes? $X$ is binomial $(n,p)$
    \item $L$ trials until and up to $k$ observations of desired outcomes? $L$ is Pascal $(k,p)$
\end{itemize}
    \item If \textbf{NO}
        \begin{itemize}
    \item Single experiment and multiple likely outocmes? Uniform
    \item Relates to rate of arrivals and occurence? Poisson 
    \item Some other PMF
\end{itemize}
\end{itemize}

\subsection{Cumulative Distribution Function} The CDF of a random variable $X$ is 
\[
F_X(x) = P(X \leq x) 
\]
The CDF of a random variable $X$ is the probability that $X$ is less than or equal to $x$
\subsection{Problem 5.1}
\textbf{Problem:} You roll a six sided die. Let $X$ be the random variable denoting the number of dots that appear. Find $F_X(x)$ and plot it.
\[
    F_X=x = x/6, \hspace{3mm} 1 \leq x \leq 6 \hspace{3mm}, \text{0 otherwise}
\]
\begin{center}
\begin{tikzpicture}
    \begin{axis}
        \addplot+[only marks,
            scatter]
            table[meta=FxX]
            {./Data/diceCDF.dat};
    

    \end{axis}

\end{tikzpicture}
\end{center}
\subsection{Expected Value and Variance}
\[
    E(X) = \sum_{x \in S_x} xP(x)
\]
\[
    E(X^2) = \sum_{x \in S_x} x^2P(x)
\]

\[
Var (X) = E(X - E(X)^2) = E(X^2) - E(X)^2 
\]
\[
    s = \sqrt{Var(X)}
\]
\subsection{Continuous Random Variables}
\subsubsection{Discrete Wheel Example}
Assume you have a wheel with a pointer that can only move in 90 degree increments. $\theta_m = 90$ and $S_\theta = \{90,180,270,360\}$. With $P(\theta_i) = 1/4$, draw the cdf. (Imagine a staircase plot here) 
\begin{center}
\begin{tikzpicture}
    \begin{axis}
        \addplot+[only marks,
            ]
            table[meta=P]
            {./Data/DiscreteWheel.dat};

    \end{axis}

\end{tikzpicture}
\end{center}
\subsubsection{Moving to continuous}
When $\theta_m$ gets infinitismally small, $\theta$ takes on a continum of values and becomes continuous.

\subsubsection{Definition}
When a random variable $X$ takes on a continoum of real values, $X$ is a continuous random variable with a continuous random variable with a continuous CDF. CDF of $X$ is continuous for a continuous random variable $X$.
\[
    \text{CDF:} \hspace{3mm} F_X(x) = P(X \leq x)
\]
Continuous CDF follows the rules:  
\begin{itemize}
    \item $F_X(-\infty) = 0$
    \item $F_X(\infty) = 1$
    \item $P(x_1 \leq X \leq x_2) = F_X(x_2) - F_X(x_1)$
    \item $P(X = x) = 0$. 
\end{itemize}

\subsection{Probability Density Function}
\[
p_1 = P(x_1 < X < x_1 + \Delta) = F_X(x_1 + \Delta) - F_X(x_1)
\]
\[
    p_1 = P(x_1 < X < x_1 + \Delta) = \frac{\Delta(F_X(x_1 + \Delta) - F_X(x_1))}{\Delta}
\]
\[
    p_1 = P(\text{``X is near } x_1 \text{"} ) f_x(x_1)
\]
\[
f_X(x) = \frac{dF_X(x)}{dx}
\]

\subsection{PDF Therorems}
\[
    f_X(x) \geq 0, \hspace{3mm} \forall x
\]
\[
    F_X(x) = \int_{-\infty}{x} f_x(u)du
\]
\[
    \int_{-\infty}^{\infty} f_X(x)dx = 1
\]
\subsection{Notable Concept}
\[
    P(x_1 \leq X \leq x_2) = \int_{x_1}^{x_2} f_X(x)dx
\]
We want to know that $X$ is in a range of values
\subsection{Problem 7.1} 
The CDF of a random variable $Y$ is 
\[
    F_Y(y) = \begin{cases}
        0, & y< 0 \\
        y^3, & 0 \leq y \leq 1 \\
        1, & y > 1
    \end{cases}
\]
PDF of $Y$ would be 
\[
    f_Y(y) = \begin{cases} 
        0, & y < 0 \\
        3y^2 & 0 \leq y \leq 1 \\
        0 & y > 1
    \end{cases}
\]
\includegraphics[scale=0.5]{./Data/example71.png} \newline
Then $P(1/4 < Y \leq 3/4)$ would be equal to 
\[
    \int_{1/4}^{3/4} 3y^2 dy = \bigg(\frac{3}{4} \bigg)^3 - \bigg(\frac{1}{4} \bigg)^3 = \frac{26}{64} = \frac{13}{32}
\]
\subsection{Problem 7.2} 
For a constant parameter $a > 0$ a Rayleigh random variable has the PDF $F_X(x) = a^2xe^{(-a^2x^2)/2}$ for $x > 0$. What is the CDF of $X$.
\[
\int_{0}^{x} a^2xe^{-a^2x^2/2} 
\]
Using the substitution $u = a^2x^2$ we then get the integral
\[
    \int e^{-u/2}du = -e^{-u/2}
\]
Reverting the substitution
\[
    e^{-u/2} = e^{-a^2x^2/2} 
\]
Then applying the limits of integration gets us our CDF. 
\[
    F_X(x) = e^{0} - e^{-a^2x^2/2} = 1 - e^{-a^2x^2/2} , \hspace{3mm} x \geq 0 
\]

\subsection{Expected Value of Continuous Random Variables}
\[
    E(X) = \int_{-\infty}^{\infty} xf_X(x)dx
\]
The same theorems for Expected value apply :) 
\begin{itemize} 
    \item $E(X - \mu x) = 0$
    \item $E(aX + b) = aE(X) + b)$
    \item $Var(X) = E[(X - \mu X)^2] = E(X^2) - E(X)^2 $
    \item $E(X^2) = \int_{-\infty}^{\infty} x^2f_X(x)dx$
    \item $E[(X - \mu x)^2] = \int_{-\infty}^{\infty} (x - \mu x)^2 f_X(x) dx$
\end{itemize}
\subsection{Problem 7.3}
The PDF of a random variable $Y$ is 
\[
    f_Y(y) = \begin{cases} 
        \frac{3y^2}{2} & -1 \leq y \leq 1 \\
        0 & \text{otherwise} \\
    \end{cases}
\]
\subsubsection{Expected Value of Y}
\[
    E(Y) = \int_{-1}^{1} yf_Y(y) = \int_{-1}^{1} \frac{3y^3}{2} = \frac{3}{8}y^4 \bigg|^{1}_{-1} = 0
\]
\subsubsection{Second Moment of Y}
\[
    E(Y^2) = \int_{-1}^{1} y^2f_Y(y) = \int_{-1}^{1} \frac{3y^4}{2} = \frac{3}{10}y^5 \bigg|^{1}_{-1} = \frac{6}{10} = 0.6
\]
\[
    s = \sqrt{Var(Y)} =  \sqrt{E(X^2) - E(X)^2} = \sqrt{0.6} \approx 0.775
\]
\newpage

\section{Week 5}
meow
\end{document}
